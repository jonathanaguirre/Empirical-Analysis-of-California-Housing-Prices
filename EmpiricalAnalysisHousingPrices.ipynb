{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4f9080bf-fad3-4be7-a926-0596ade85505",
      "cell_type": "markdown",
      "source": "# Introduction",
      "metadata": {}
    },
    {
      "id": "faaa017c-e02c-40a1-aa17-6a6b3506005b",
      "cell_type": "markdown",
      "source": "Let's predict the cost of homes in parts of California using Python, specificilly the Scikit-learn library. Nine Feature variables and one Target variable will be utlized. Linear Regresson, Random Forest Regressor, and XGBoost will be used in Model Training. Accuracy, R\n\n## Target Variable\nmedianHouseValue: Median house value for households within a block (measured in US Dollars).\n\n## Feature Variables\nlongitude: A measure of how far west a house is (a higher value is farther west).\nlatitude: A measure of how far north a house is (a higher value is farther north).\nhousingMedianAge: Median age of a house within a block (a lower number implies a newer building).\ntotalRooms: Total number of rooms within a block.\ntotalBedrooms: Total number of bedrooms within a block.\npopulation: Total number of people residing within a block.\nhouseholds: Total number of households. where a household unit is comprised of a group of people for some block.\nmedianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars).\noceanProximity: Location of the house in proximity to the ocean.\n\n## Background\nScikit-learn is a machine learning library for the Python. It features various classification, regression, and cluster learning algorithms.\n\n## Dependencies\npandas - To work with solid data-structures, n-dimensional matrices and perform exploratory data analysis.\nmatplotlib - To visualize data using 2D plots.\nseaborn - To make 2D plots look pretty and readable.\nscikit-learn - To create machine learning models easily and make predictions.\nnumpy - To work with arrays.",
      "metadata": {}
    },
    {
      "id": "c03c3c2d-0b27-4f59-bb96-033e8843bd7d",
      "cell_type": "markdown",
      "source": "# Obtain & Load Data",
      "metadata": {}
    },
    {
      "id": "096711ea-5936-418f-92e1-92f6d8de95b0",
      "cell_type": "code",
      "source": "# Load packages\n%pip install pandas\n%pip install matplotlib\n%pip install seaborn\n%pip install scikit-learn\n%pip install numpy\n%pip install xgboost\n%matplotlib inline\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split,RepeatedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "debfa95b-298e-491e-beb3-6c5a8c073811",
      "cell_type": "code",
      "source": "# Load the Dataset.\nhousing_df = pd.read_csv('data/housing.csv')\n\n# Use .info() to show the features (i.e. columns) in your dataset along with a count and datatype.\nhousing_df.info()",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a93a7026-ac7a-4c0d-801a-c8baf953dbd8",
      "cell_type": "code",
      "source": "# Use .shape to understand how many observations (ie rows/records) of the dataset.\n# The shape of a DataFrame is a tuple of array dimensions that tells the number of rows and columns of a given DataFrame.\n# (row count, column count).\nhousing_df.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a88b712f-aba4-474b-8018-646a4e79f452",
      "cell_type": "code",
      "source": "# Using .head() function to view the first few observations (i.e. rows/records) of the dataset.\nhousing_df.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96c97bcb-70ca-4e5f-a3ee-00dda4c68d4e",
      "cell_type": "code",
      "source": "# Using .tail() function to view the last few observations (i.e. records) of the dataset.\nhousing_df.tail()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9914dcbc-a621-4d75-aad2-3b73810eb705",
      "cell_type": "code",
      "source": "# Use .describe() to see metrics about the dataset (count, mean, std, min, 25%, 50%, 75%, max).\n# Count is the count of non-null observations (i.e. rows).\n# Mean is the average of values for the given column.\n# Std is standard deviation - how far off from the mean the values are.\n# Min is the minimum amount of the value.\n# 25% is the 25th percentile.\n# 50% is the 50th percentile.\n# 75% is the 75th percentile.\n# max is the maximum amount of the value.\nhousing_df.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b4aa5d2c-b4a1-4407-83e1-6daad3fe2026",
      "cell_type": "markdown",
      "source": "# Prepare & Preprocess Data",
      "metadata": {}
    },
    {
      "id": "4b3b476a-76a7-44f1-b86e-0e8be849f35c",
      "cell_type": "markdown",
      "source": "## Understand Missing Data",
      "metadata": {}
    },
    {
      "id": "8d25bbb4-6e2e-46aa-8a76-6c82b931efa8",
      "cell_type": "code",
      "source": "# Check for missing values.\nhousing_df.isnull().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c03c1c78-397c-462c-8678-a0eb73333884",
      "cell_type": "code",
      "source": "# Calculate the % of missing data.\nhousing_df['total_bedrooms'].isnull().sum()/housing_df.shape[0] * 100",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "25260e50-6270-487c-ae41-b6649c2f5998",
      "cell_type": "markdown",
      "source": "## Use Imputation to Handle Missing Data",
      "metadata": {}
    },
    {
      "id": "830d3552-f92a-4cb9-91e7-8a4cce1e4b6e",
      "cell_type": "code",
      "source": "# create a temporary copy of the dataset.\nhousing_df_temp = housing_df.copy()\n\n# retrieve columns with numerical data; will exclude the ocean_proximity column since the datatype is object; other columns are float64.\ncolumns_list = [col for col in housing_df_temp.columns if housing_df_temp[col].dtype != 'object']\n\n# extract columns that contain at least one missing value.\nnew_column_list = [col for col in housing_df_temp.loc[:, housing_df_temp.isnull().any()]]\n\n# update temp dataframe with numeric columns that have empty values.\nhousing_df_temp = housing_df_temp[new_column_list]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a3d19dee-1895-4897-93bb-caff09808b51",
      "cell_type": "markdown",
      "source": "## Impute Missing Data Using Machine Learning",
      "metadata": {}
    },
    {
      "id": "96165cfc-9b18-4e1a-bcae-d4fed45d933a",
      "cell_type": "code",
      "source": "# initialize KNNImputer to impute missing data using machine learning.\nknn = KNNImputer(n_neighbors = 3)\n\n# fit function trains the model.\nknn.fit(housing_df_temp)\n\n# transform the data using the model then apply the transformation model (ie knn) to the data.\narray_Values = knn.transform(housing_df_temp)\n\n# convert the array values to a dataframe with the appropriate column names.\nhousing_df_temp = pd.DataFrame(array_Values, columns = new_column_list)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c85fcb2e-9866-4f2e-bc40-e887138a59dd",
      "cell_type": "code",
      "source": "# confirm there are no columns with missing values.\nhousing_df_temp.isnull().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "09711fec-4e06-44be-bf44-8a956310d88b",
      "cell_type": "code",
      "source": "# overlay the imputed column over the old column with missing values then loop through the list of columns and overlay each one.\nfor column_name in new_column_list:\n    housing_df[column_name] = housing_df_temp.replace(housing_df[column_name],housing_df[column_name])\n\n# confirm columns no longer contain null data.\nhousing_df.isnull().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9ad19be-0d32-41cd-b0f9-ff769d530e25",
      "cell_type": "markdown",
      "source": "# Understand the Relationship: Target Variable and Features",
      "metadata": {}
    },
    {
      "id": "bf789181-77d4-4dcf-a483-a33ac13977d3",
      "cell_type": "markdown",
      "source": "## Histograms",
      "metadata": {}
    },
    {
      "id": "033d6729-ed09-4817-9595-221b5e79b7a2",
      "cell_type": "code",
      "source": "# Plot the distribution of the target variable (median_house_value) using a histogram.\n\n# bins->amount of columns.\nplt.hist(housing_df['median_house_value'], bins=80)\nplt.xlabel(\"House Values\")\n\n# We expect the plot to show Median House Values are distributed normally with some outliers. \n# Observe homes fall within th $100,000-$200,000 USD range.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e677ea95-92a4-49b8-b61d-7055e3dc734d",
      "cell_type": "code",
      "source": "# Consider histograms for the all features so that we can understand data distribution.\n# Use housing_df as we do not want to plot the String Datatype, OCEAN_PROXIMITY. This will be encoded to a Numeric datatype later.   \nhousing_df.hist(bins=50, figsize=(20,15))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "71506a49-757b-4d48-b1f9-105453c946a0",
      "cell_type": "markdown",
      "source": "## Use a heatmap to show correlation",
      "metadata": {}
    },
    {
      "id": "e0cc0a06-2597-4168-b37c-210f8944ed8e",
      "cell_type": "code",
      "source": "# Plot a graphical correlation matrix for each pair of columns in the dataframe.\n# Below is the data frame correlation function.\ncorr = housing_df.corr(numeric_only=True)\nprint(corr)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ef913e30-4e12-4813-83fa-d58a26bc213c",
      "cell_type": "code",
      "source": "# Create the heatmap with a set size and print to screen.\nplt.figure(figsize = (8,8))\nsns.heatmap(corr, annot=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5ee2d3e-428a-4df7-9ddd-481d15e31def",
      "cell_type": "markdown",
      "source": "## Feature Engineering",
      "metadata": {}
    },
    {
      "id": "69cda055-4bb5-47fb-b1be-8079d38bae47",
      "cell_type": "code",
      "source": "# Given we have very high correlation among some features, we need to manipulate data.\n# Observe from the heatmap that total_rooms, total_bedrooms, population, households have high correlation.\n# Could there be a change in model performance if we create ratio's of these features?\n\n#  New Ratio: total rooms to households.\nhousing_df['rooms_per_household'] = housing_df['total_rooms']/housing_df['households']\n\n# New Ratio: total bedrooms to the total rooms.\nhousing_df['bedrooms_per_room'] = housing_df['total_bedrooms']/housing_df['total_rooms']\n\n# New Ratio: population to the households.\nhousing_df['population_per_household']= housing_df['population']/housing_df['households']\n\n# New Ratio: latitude and longitude.\nhousing_df['coords'] = housing_df['longitude']/housing_df['latitude']\n\nhousing_df.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13149f32-7f2e-43be-add7-28557fd9d609",
      "cell_type": "code",
      "source": "# Remove total_rooms, households, total bedrooms, popluation, longitude, latitude as these now have a ratio.\nhousing_df = housing_df.drop('total_rooms', axis=1)\nhousing_df = housing_df.drop('households', axis=1)\nhousing_df = housing_df.drop('total_bedrooms', axis=1)\nhousing_df = housing_df.drop('population', axis=1)\nhousing_df = housing_df.drop('longitude', axis=1)\nhousing_df = housing_df.drop('latitude', axis=1)\n\nhousing_df.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d2b8ab3-4b4e-49b1-9fd7-31e43f257111",
      "cell_type": "markdown",
      "source": "## Heatmap after removing correlation",
      "metadata": {}
    },
    {
      "id": "550733ff-36e5-4f00-b1c3-46e0f60c079a",
      "cell_type": "code",
      "source": "corr = housing_df.corr(numeric_only=True) \n\n# Increase the size of the heatmap.\nplt.figure(figsize = (7,7))\n\nsns.heatmap(corr, annot=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f04b9d35-8b39-47b3-9080-cf6e70453662",
      "cell_type": "markdown",
      "source": "# String Datatype to Numeric Datatype",
      "metadata": {}
    },
    {
      "id": "28f7fe9b-1cd6-42e9-b963-45997f583b07",
      "cell_type": "code",
      "source": "# Note ocean_proximity is the only categorical data type.\nhousing_df.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b04f1938-bb8c-45bd-bf04-3bcfa57a38a3",
      "cell_type": "code",
      "source": "# What unique categories exist for ocean_proximity?\nhousing_df.ocean_proximity.unique()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1443f0bc-4bcd-40f7-ac83-dbfc05c25952",
      "cell_type": "code",
      "source": "# How many entries exist per category?\nhousing_df[\"ocean_proximity\"].value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8eecf65b-9780-4e54-8997-3f754a0f4682",
      "cell_type": "markdown",
      "source": "## One-Hot Encoding",
      "metadata": {}
    },
    {
      "id": "350db4a3-2144-45b6-9bc2-de87d4181e3e",
      "cell_type": "code",
      "source": "# Let's see how the Panda's get_dummies() function works. If a cell contains data, then a True is returned, otherwise False.\nprint(pd.get_dummies(housing_df['ocean_proximity']))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a4900cb-d257-4a4f-a1d9-266357adcc2a",
      "cell_type": "code",
      "source": "# Replace the ocean_proximity column using get_dummies(), then print to screen. Note ocean_proximity column is gone.\nhousing_df_encoded = pd.get_dummies(data=housing_df, columns=['ocean_proximity'])\nhousing_df_encoded.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "89a3438c-588e-4a4a-9f52-a778b01559d9",
      "cell_type": "markdown",
      "source": "# Prepare for Model Training",
      "metadata": {}
    },
    {
      "id": "f9a22602-2585-4269-b1ef-9b83aac2e621",
      "cell_type": "code",
      "source": "# remove spaces from column names, convert cell data to lowercase, and remove all special characters.\nhousing_df_encoded.columns = [c.lower().replace(' ', '_').replace('<', '_') for c in housing_df_encoded.columns]\n\n# Split target variable and feature variables\nX = housing_df_encoded[['housing_median_age', 'median_income','bedrooms_per_room','population_per_household','coords','ocean_proximity__1h_ocean',\n                        'ocean_proximity_inland','ocean_proximity_island','ocean_proximity_near_bay','ocean_proximity_near_ocean']]\ny = housing_df_encoded['median_house_value']\n\nprint(X)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2eab62a-fdac-4397-aa69-347040cf280f",
      "cell_type": "markdown",
      "source": "## Split Training and Test Data",
      "metadata": {}
    },
    {
      "id": "b3041868-eed7-42a1-9869-ee2906dfee4e",
      "cell_type": "code",
      "source": "# Using Numpy arrarys, split the data into Training (70%) and Testing (30%) sets.\n# X -> array with the inputs; y -> array of the outputs\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True, test_size=0.3)\n\n# Confirm how the data was split\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d36a5243-c80e-4ebc-8637-fa3c3dafd849",
      "cell_type": "markdown",
      "source": "# Model Training",
      "metadata": {}
    },
    {
      "id": "d6056e6b-aab7-4ef5-9b99-a5d3242153b9",
      "cell_type": "markdown",
      "source": "## Linear Regression ",
      "metadata": {}
    },
    {
      "id": "5cd1aa69-69af-4243-a007-8d4ffe872eb2",
      "cell_type": "markdown",
      "source": "### Create, Train, and Run",
      "metadata": {}
    },
    {
      "id": "188272c9-f673-4f0d-b36b-e18fa62b03c9",
      "cell_type": "code",
      "source": "# Scikit-learn’s LinearRegression will train the model on both the training dataset, then \n# evaluate on the test dataset.\n\n# Create a Linear regressor using all the feature variables.\nreg_model = LinearRegression()\n\n# Train the model using the training sets.\nreg_model.fit(X_train, y_train)\n\n# Run the predictions on the training and testing data.\ny_pred_test = reg_model.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9da29a32-76e9-4a1c-a773-221f864a0370",
      "cell_type": "markdown",
      "source": "### Evaluate the model",
      "metadata": {}
    },
    {
      "id": "960bce5c-5fe7-4e5d-a805-ca23ae57d32f",
      "cell_type": "code",
      "source": "# Recall 'medianHouseValue' is the target. Note the Percent Difference between the two values. Are these predicted values acceptable to you? \npred_test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test, '% Difference': abs((y_test - y_pred_test) / ((y_test + y_pred_test)) / 2 * 100)})\n\npred_test_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d068aa9-3a1c-4025-b57f-2718f927c214",
      "cell_type": "markdown",
      "source": "### Accuracy and Error",
      "metadata": {}
    },
    {
      "id": "acddf85a-d52e-43bf-843a-255d4d7039fa",
      "cell_type": "code",
      "source": "# Let's continue model evaluation by using the 𝑅^2 Metric. A score between 0 and 1 will represent how well the model is performing.\nr2_reg_model_test = round(reg_model.score(X_test, y_test),2)\n\nprint(\"R^2 Test: {}\".format(r2_reg_model_test))\n\n# Determine the Root Mean Squared Error (RMSE) on the test data.\nprint('RMSE on test data: ',  mean_squared_error(y_test, r2_reg_model_test)**(0.5))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "82b2dc94-a962-4e80-8dfe-a5b0e307120b",
      "cell_type": "markdown",
      "source": "## RandomForestRegressor",
      "metadata": {}
    },
    {
      "id": "63eefccc-292a-439b-9c14-9b2a5019602c",
      "cell_type": "markdown",
      "source": "### Create, Train, and Run",
      "metadata": {}
    },
    {
      "id": "0bb50548-51b0-445c-91fb-9ab8ec703048",
      "cell_type": "code",
      "source": "# Let's use another Machine Learning algorithm, Random Forest. Scikit-learn’s RandomForestRegressor will train the \n# model on both training sets, and then evaluate using test sets.\n\n# Create a regressor using all the feature variables avaible in the training sets. \nrf_model = RandomForestRegressor(n_estimators=10,random_state=10)\n\n# Train the model using the training sets.\nrf_model.fit(X_train, y_train)\n\n# Run the predictions on the training and testing data for the RandomForestRegressor algorithm.\ny_rf_pred_test = rf_model.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a239d25-bde8-4067-a16d-b0f33ddb7dba",
      "cell_type": "markdown",
      "source": "### Evaluate the Model",
      "metadata": {}
    },
    {
      "id": "1600e050-c55a-4db4-81c9-ca801d4bb610",
      "cell_type": "code",
      "source": "# Recall 'medianHouseValue' is the target. Note the Percent Difference between the two values. Are these predicted values acceptable to you? \nrf_pred_test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_rf_pred_test, '% Difference': abs((y_test - y_rf_pred_test) / ((y_test + y_rf_pred_test)) / 2 * 100)})\n\nrf_pred_test_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f9d85530-fea3-4d3b-829e-be19996cb928",
      "cell_type": "markdown",
      "source": "### Accuracy and Error",
      "metadata": {}
    },
    {
      "id": "cece6247-9359-4650-bda7-04ad735ee8f1",
      "cell_type": "code",
      "source": "# Determine accuracy uisng the 𝑅^2 metric.\nscore = r2_score(y_test, y_rf_pred_test)\n\nprint(\"R^2 - {}%\".format(round(score, 2) *100))\n\n# Determine the Root Mean Squared Error (RMSE) on the test data.\nprint('RMSE on test data: ',  mean_squared_error(y_test, y_rf_pred_test)**(0.5))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "be83613a-d597-499a-8b75-ee3440b33b0f",
      "cell_type": "markdown",
      "source": "### Feature Importance",
      "metadata": {}
    },
    {
      "id": "536e3ca3-e56b-4508-99fc-01a02b9bb917",
      "cell_type": "code",
      "source": "# The RandomForestRegressor package has an additional option for determining feature importance for all dataset varibles.\n# The 6 most imporant features will be plotted.\nplt.figure(figsize=(10,6))\nfeat_importances = pd.Series(rf_model.feature_importances_, index = X_train.columns)\nfeat_importances.nlargest(6).plot(kind='barh');",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b078552b-74ca-4436-b98b-465f6c0ef995",
      "cell_type": "markdown",
      "source": "#### Feature Selection",
      "metadata": {}
    },
    {
      "id": "35f88d4a-65cd-485c-9e35-34795a06267e",
      "cell_type": "code",
      "source": "# Now that we have determined feature importance, let's narrow to 6 features of the orginal 10 features for learning.\n# Note that we are not creating any new ratio's at this moment.\ntrain_x_if = X_train[['bedrooms_per_room', 'housing_median_age', 'coords', 'ocean_proximity_inland','population_per_household','median_income']]\ntest_x_if = X_test[['bedrooms_per_room', 'housing_median_age', 'coords', 'ocean_proximity_inland','population_per_household','median_income']]\n\n# Create an object of the RandfomForestRegressor Model.\nrf_model_if = RandomForestRegressor(n_estimators=10,random_state=10)\n\n# Fit the model with the training dataset.\nrf_model_if.fit(train_x_if, y_train)\n\n# Predict the target on the test data.\npredict_test_with_if = rf_model_if.predict(test_x_if)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "152c1ea6-7dd4-4480-8da6-7441b1f8845d",
      "cell_type": "markdown",
      "source": "#### Recalculate RMSE",
      "metadata": {}
    },
    {
      "id": "58b292bc-47cd-4b8b-9321-633422e50e89",
      "cell_type": "code",
      "source": "# Determine the Root Mean Squared Error (RMSE) on the test data.\nprint('RMSE on test data: ',  mean_squared_error(y_test, predict_test_with_if)**(0.5))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9040088a-8fef-442a-8edc-a3d09ea786a0",
      "cell_type": "markdown",
      "source": "## XGBoost",
      "metadata": {}
    },
    {
      "id": "6b94fd66-5385-43bf-a0b5-7534fdb40cda",
      "cell_type": "markdown",
      "source": "### Create, Train and Run",
      "metadata": {}
    },
    {
      "id": "246dc25c-1c95-4ab0-b946-9f471ddc1c40",
      "cell_type": "code",
      "source": "# Extreme Gradient Boosting (XGBoost) is an open-source library that provides \n# an efficient and effective implementation of the gradient boosting algorithm.\n# Use the scikit-learn wrapper classes: XGBRegressor and XGBClassifier. <----WHY?\n\n# Create the varible representing the XGBoost algorithm.\nxgb_model = XGBRegressor()\n\n# Train the model using the Training datasets.\nxgb_model.fit(X_train, y_train)\n\n# Run the predictions on the Training and Testing datasets.\ny_xgb_pred_test = xgb_model.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "87215b6e-9ee4-4332-af46-9bb835f4e932",
      "cell_type": "markdown",
      "source": "### Evaluate the Model",
      "metadata": {}
    },
    {
      "id": "ed8121b9-aedc-4a91-8da3-cce8620592ea",
      "cell_type": "code",
      "source": "# Recall 'medianHouseValue' is the target. Note the Percent Difference between the two values. Are these predicted values acceptable to you? \nxgb_pred_test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_xgb_pred_test, '% Difference': abs((y_test - y_xgb_pred_test) / ((y_test + y_xgb_pred_test)) / 2 * 100)})\n\nxgb_pred_test_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bb9abb2f-fe44-4f5b-9745-f6c3c610ef14",
      "cell_type": "code",
      "source": "fig= plt.figure(figsize=(8,8))\nxgb_pred_test_df = xgb_pred_test_df.reset_index()\nxgb_pred_test_df = xgb_pred_test_df.drop(['index'],axis=1)\nplt.plot(xgb_pred_test_df[:50])\nplt.legend(['Actual value','Predicted value'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "083c8580-b87d-4ac5-a066-7248ea970f51",
      "cell_type": "markdown",
      "source": "### Accuracy and Error",
      "metadata": {}
    },
    {
      "id": "bdf87863-636e-4f88-af01-024eb6229dc8",
      "cell_type": "code",
      "source": "# Determine accuracy uisng the 𝑅^2 metric.\nscore = r2_score(y_test, y_xgb_pred_test)\n\nprint(\"R^2 - {}%\".format(round(score, 2) *100))\n\n# Determine mean square error and root mean square error then print to screen.\nmse = mean_squared_error(y_test, y_xgb_pred_test)\nrmse = math.sqrt(mean_squared_error(y_test, y_xgb_pred_test))\n\nprint(mse)\nprint(rmse)\n\n# Calculate Mean Absolute Error\nprint(mean_absolute_error(y_test, y_xgb_pred_test))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4fbca31c-54ec-406b-be3a-aafab1cacd2d",
      "cell_type": "markdown",
      "source": "### Hyperparameter Tuning",
      "metadata": {}
    },
    {
      "id": "7cf4c874-9ef1-4a41-ab91-06e2ac65ac97",
      "cell_type": "code",
      "source": "# Determine what Hyperparameters are available for tuning.\nxgb_model.get_params()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2df4ded7-f049-4241-8910-ae69d075ddeb",
      "cell_type": "code",
      "source": "# These Hyperparameters will take on new values. Reminder, this is XGBoost specific, check varible names.\nxgb_model_2 = XGBRegressor(\n    gamma=0.05,\n    learning_rate=0.01,\n    max_depth=6,\n    n_estimators=1000,\n    n_jobs=16,\n    objective='reg:squarederror',\n    subsample=0.8,\n    scale_pos_weight=0,\n    reg_alpha=0,\n    reg_lambda=1,\n    verbosity=1)\n\nxgb_model_2.fit(X_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e52fec3a-043d-45cf-aabb-f524713a4c6a",
      "cell_type": "code",
      "source": "# Let's rerun the newly tuned hyperparameters using the same training and test datasets.\n\ny_xgb_2_pred_test = xgb_model_2.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6fac3519-26dc-4d83-95cb-30f936c137af",
      "cell_type": "code",
      "source": "# Compare the actual values (ie, target) with the values predicted by the model\nxgb_2_pred_test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_xgb_2_pred_test})\nxgb_2_pred_test_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_xgb_2_pred_test, '% Difference': abs((y_test - y_xgb_2_pred_test) / ((y_test + y_xgb_2_pred_test)) / 2 * 100)})\n\nxgb_2_pred_test_df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "94de9b05-a9ba-40f4-bfd4-94cace607f19",
      "cell_type": "code",
      "source": "# Let's vizualize actual values and predicted values within the XGBoost.\nfig= plt.figure(figsize=(8,8))\nxgb_2_pred_test_df = xgb_2_pred_test_df.reset_index()\nxgb_2_pred_test_df = xgb_2_pred_test_df.drop(['index'],axis=1)\nplt.plot(xgb_2_pred_test_df[:50])\nplt.legend(['Actual value','Predicted value'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8b67a99c-8d5e-46d8-abab-62de7d281808",
      "cell_type": "markdown",
      "source": "#### Post-Tuning: Accuracy and Error ",
      "metadata": {}
    },
    {
      "id": "9b6c5bfb-a14c-4694-96f6-3bc88cd6f468",
      "cell_type": "code",
      "source": "# Determine accuracy uisng 𝑅^2.\nr2_xgb_model_2_test = round(xgb_model_2.score(X_test, y_test),2)\n\nprint(\"R^2 Test: {}\".format(r2_xgb_model_2_test))\n\n# Determine mean square error and root mean square error then print to screen.\nmse = mean_squared_error(y_test, y_xgb_2_pred_test)\nrmse = math.sqrt(mean_squared_error(y_test, y_xgb_2_pred_test))\n\nprint(mse)\nprint(rmse)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "211eeb31-d4e5-4eba-840e-9372cf18d1e7",
      "cell_type": "markdown",
      "source": "# Cross Validation",
      "metadata": {}
    },
    {
      "id": "f1f6673c-6087-4de3-b19d-d50c6cb715c1",
      "cell_type": "code",
      "source": "# We can build and score a model on multiple folds using cross validation.\n\n# Define a model evaluation method.\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nscores = cross_val_score(xgb_model, X, y, scoring='r2', error_score='raise', cv=cv, n_jobs=-1, verbose=1)\n\n# Average of all the r2 scores across all runs.\nprint(scores.mean())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}